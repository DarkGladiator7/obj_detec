{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978bd7a7-016b-43f4-a5e4-f0638073f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d670264-6483-4c67-8e18-09fe7964f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.ops.boxes import box_iou\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import xml.etree.ElementTree as ET\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affe1d00-cc21-4528-95af-4b986cb9d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def pascal_voc_to_coco(voc_dir, coco_dir):\n",
    "    if not os.path.exists(coco_dir):\n",
    "        os.makedirs(coco_dir)\n",
    "\n",
    "    coco_annotations = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "\n",
    "    category_mapping = {}\n",
    "    annotation_id = 1\n",
    "\n",
    "    for xml_file in os.listdir(voc_dir):\n",
    "        if not xml_file.endswith('.xml'):\n",
    "            continue\n",
    "\n",
    "        tree = ET.parse(os.path.join(voc_dir, xml_file))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        filename = root.find('filename').text\n",
    "        image_path = os.path.join(voc_dir, filename)\n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        image_id = len(coco_annotations[\"images\"]) + 1\n",
    "\n",
    "        coco_annotations[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": filename,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "\n",
    "        image.save(os.path.join(coco_dir, filename))\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            category = obj.find('name').text\n",
    "            if category not in category_mapping:\n",
    "                category_id = len(category_mapping) + 1\n",
    "                category_mapping[category] = category_id\n",
    "                coco_annotations[\"categories\"].append({\n",
    "                    \"id\": category_id,\n",
    "                    \"name\": category\n",
    "                })\n",
    "            else:\n",
    "                category_id = category_mapping[category]\n",
    "\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "            coco_annotations[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [xmin, ymin, xmax - xmin, ymax - ymin],\n",
    "                \"area\": (xmax - xmin) * (ymax - ymin),\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    with open(os.path.join(coco_dir, 'annotations.json'), 'w') as f:\n",
    "        json.dump(coco_annotations, f, indent=4)'''\n",
    "\n",
    "#Modified code to handle VOC_2007 xml structure\n",
    "def pascal_voc_to_coco(voc_annotation_dir, voc_image_dir, coco_dir, max_files=10):\n",
    "    if not os.path.exists(coco_dir):\n",
    "        os.makedirs(coco_dir)\n",
    "\n",
    "    coco_annotations = {\"images\": [], \"annotations\": [], \"categories\": []}\n",
    "    category_mapping = {}\n",
    "    annotation_id = 1\n",
    "\n",
    "    xml_files = [f for f in os.listdir(voc_annotation_dir) if f.endswith('.xml')]\n",
    "    print(f\"üîç Found {len(xml_files)} XML annotation files.\")\n",
    "\n",
    "    if len(xml_files) == 0:\n",
    "        print(\"‚ùå No XML files found. Check your VOC dataset path.\")\n",
    "        return\n",
    "\n",
    "    for idx, xml_file in enumerate(xml_files):\n",
    "        xml_path = os.path.join(voc_annotation_dir, xml_file)\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error parsing {xml_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        filename = root.find('filename').text  # e.g., '000005.jpg'\n",
    "        \n",
    "        # Check if the image exists\n",
    "        image_path = None\n",
    "        for ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            temp_path = os.path.join(voc_image_dir, filename.replace(\".jpg\", ext))\n",
    "            if os.path.exists(temp_path):\n",
    "                image_path = temp_path\n",
    "                break\n",
    "\n",
    "        if image_path is None:\n",
    "            print(f\"‚ö†Ô∏è Warning: Image {filename} not found in {voc_image_dir}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Load image size\n",
    "        image = Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        image_id = idx + 1\n",
    "\n",
    "        coco_annotations[\"images\"].append({\n",
    "            \"id\": image_id, \"file_name\": filename, \"width\": width, \"height\": height\n",
    "        })\n",
    "\n",
    "        image.save(os.path.join(coco_dir, filename))  # Save images in COCO folder\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            category = obj.find('name').text\n",
    "            if category not in category_mapping:\n",
    "                category_id = len(category_mapping) + 1\n",
    "                category_mapping[category] = category_id\n",
    "                coco_annotations[\"categories\"].append({\"id\": category_id, \"name\": category})\n",
    "            else:\n",
    "                category_id = category_mapping[category]\n",
    "\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin, ymin, xmax, ymax = (int(bndbox.find(pos).text) for pos in ['xmin', 'ymin', 'xmax', 'ymax'])\n",
    "\n",
    "            if xmax <= xmin or ymax <= ymin:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid bbox in {xml_file}: [{xmin}, {ymin}, {xmax}, {ymax}]\")\n",
    "                continue\n",
    "\n",
    "            coco_annotations[\"annotations\"].append({\n",
    "                \"id\": annotation_id, \"image_id\": image_id, \"category_id\": category_id,\n",
    "                \"bbox\": [xmin, ymin, xmax - xmin, ymax - ymin], \"area\": (xmax - xmin) * (ymax - ymin), \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    with open(os.path.join(coco_dir, 'annotations_test.json'), 'w') as f:\n",
    "        json.dump(coco_annotations, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Successfully converted {len(xml_files)} VOC files to COCO format.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1079db49-c728-432f-9d31-20db9f378f36",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pascal_voc_to_coco() missing 1 required positional argument: 'coco_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpascal_voc_to_coco\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_voc_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_coco_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: pascal_voc_to_coco() missing 1 required positional argument: 'coco_dir'"
     ]
    }
   ],
   "source": [
    "#pascal_voc_to_coco('path_to_voc_dir', 'path_to_coco_dir')\n",
    "pascal_voc_to_coco(annotations_dir, images_dir, coco_output_dir, max_files=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f19ed7-06f5-47e2-b15f-169ae2071810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, bilateral_filter=False, median_filter=False, gaussian_filter=False):\n",
    "    \"\"\"\n",
    "    Preprocess an image by applying specified filters.\n",
    "\n",
    "    Parameters:\n",
    "        img_path (str): Path to the input image.\n",
    "        bilateral_filter (bool): Apply bilateral filter if True.\n",
    "        median_filter (bool): Apply median filter if True.\n",
    "        gaussian_filter (bool): Apply Gaussian filter if True.\n",
    "\n",
    "    Returns:\n",
    "        processed_img: The processed image after applying the filters.\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found at the specified path.\")\n",
    "\n",
    "    processed_img = img\n",
    "\n",
    "    # Apply bilateral filter\n",
    "    if bilateral_filter:\n",
    "        processed_img = cv2.bilateralFilter(processed_img, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Apply median filter\n",
    "    if median_filter:\n",
    "        processed_img = cv2.medianBlur(processed_img, ksize=5)\n",
    "\n",
    "    # Apply Gaussian filter\n",
    "    if gaussian_filter:\n",
    "        processed_img = cv2.GaussianBlur(processed_img, ksize=(5, 5), sigmaX=0)\n",
    "\n",
    "    return processed_img    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74b877-e206-4d64-8891-ecb86283c09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4970898e-12c9-4175-8386-7ccca92632a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(train=False):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.ColorJitter(p=0.2),\n",
    "            A.Transpose(p=0.2),\n",
    "            ToTensorV2()], \n",
    "            bbox_params=A.BboxParams(format='coco'))\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f78df48-9756-4d9f-bc01-2913ab0efa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class Detection(datasets.VisionDataset):\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.split = split #train, valid, test\n",
    "        self.coco = COCO(os.path.join(root, split, \"annotations.json\")) \n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "    \n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "        \n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target] \n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes)\n",
    "        \n",
    "        image = transformed['image']\n",
    "        boxes = transformed['bboxes']\n",
    "        \n",
    "        new_boxes = [] # convert from xywh to xyxy\n",
    "        for box in boxes:\n",
    "            xmin = box[0]\n",
    "            xmax = xmin + box[2]\n",
    "            ymin = box[1]\n",
    "            ymax = ymin + box[3]\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "        \n",
    "        targ = {} \n",
    "        targ['boxes'] = boxes\n",
    "        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
    "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) \n",
    "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "        return image.div(255), targ \n",
    "    def __len__(self):\n",
    "        return len(self.ids)'''\n",
    "\n",
    "class ObjectDetection(datasets.VisionDataset):\n",
    "    def __init__(self, root, split='train', transform=None):\n",
    "        self.root = root  # Root directory containing images and annotations\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(os.path.join(root, split, \"annotations.json\"))  # No train/valid split\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "        # Filter out images with no annotations\n",
    "        self.ids = [id for id in self.ids if len(self._load_target(id)) > 0]\n",
    "\n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root,self.split, path))  # Directly in root, not subdir\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)  # Returns a NumPy array\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "\n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, bboxes=boxes)\n",
    "            image = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "\n",
    "        new_boxes = []  # Convert from xywh to xyxy\n",
    "        for box in boxes:\n",
    "            xmin, ymin, w, h, category_id = box\n",
    "            xmax = xmin + w\n",
    "            ymax = ymin + h\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "\n",
    "        targ = {\n",
    "            'boxes': boxes,\n",
    "            'labels': torch.tensor([t['category_id'] for t in target], dtype=torch.int64),\n",
    "            'image_id': torch.tensor([id], dtype=torch.int64),\n",
    "            'area': (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]),\n",
    "            'iscrowd': torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        # **Convert NumPy image to PyTorch tensor & Change Shape [H, W, C] -> [C, H, W]**\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)  # Convert [H, W, C] ‚Üí [C, H, W]\n",
    "\n",
    "        return image / 255.0, targ  # Normalize image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7a8aa4-b6f2-4253-b5e5-6659b3563742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = \"/work/qucikml/VOC_2007\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fed2206-9ca0-4b2d-bd62-f24c4cfcf16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = AquariumDetection(root=dataset_path, transforms=get_transforms(True))\n",
    "#train_dataset = CocoDetection(root=dataset_path, annFile=annotations_file, transform=get_transforms(True))\n",
    "full_dataset = ObjectDetection(root=dataset_path, split=\"train\", transform=None)\n",
    "\n",
    "# Define train-validation split ratio\n",
    "val_size = int(0.2 * len(full_dataset))  # 20% for validation\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06a8f51e-5fb8-4968-821f-331d5892a8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of Classes: 21\n"
     ]
    }
   ],
   "source": [
    "'''coco = COCO(os.path.join(dataset_path, \"train\", \"annotations.json\"))\n",
    "categories = coco.cats\n",
    "n_classes = len(categories.keys())'''\n",
    "coco = COCO(os.path.join(dataset_path,\"train\", \"annotations.json\"))\n",
    "categories = coco.cats\n",
    "n_classes = len(categories.keys())+1\n",
    "\n",
    "print(f\"Number of Classes: {n_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d00f7e65-f4e0-442e-a54d-6441ec4e6ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/qucikml/quickml_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=True)\n",
    "anchor_sizes = ((32,), (64,), (128,), (256,), (512,))  \n",
    "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)  \n",
    "model.rpn.nms_thresh = 0.7\n",
    "model.roi_heads.nms_thresh = 0.5\n",
    "model.roi_heads.score_thresh = 0.05\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features \n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "209f548a-7730-45a1-b120-fe946ab6386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82ffd2a7-8641-4faf-9516-6f3251b38054",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3505f057-20cd-487c-b2a6-2e3af39ecbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ObjectDetection\n",
       "    Number of datapoints: 5011\n",
       "    Root location: /work/qucikml/VOC_2007"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5b9efbf-da80-421b-9882-85a1a158de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,targets = next(iter(train_loader))\n",
    "images = list(image for image in images)\n",
    "#targets = [{k: torch.as_tensor(v, dtype=torch.int64).to(device) for k, v in t.items()} for t in targets]\n",
    "targets = [{k:v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77567673-4d61-4a1e-97de-af15074894c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff60f5ab-0856-4e1c-a1f5-d25c761bd8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1c20ca9-ad26-4fe4-9594-9ce4949de3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(patience=3, min_delta=0.0, restore_best_weights=True, start_from_epoch=0):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    def check(epoch, val_loss, model):\n",
    "        nonlocal best_loss, patience_counter, best_model_weights\n",
    "        \n",
    "        if epoch < start_from_epoch:\n",
    "            return False\n",
    "        \n",
    "        if val_loss < best_loss - min_delta:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            if restore_best_weights:\n",
    "                best_model_weights = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                if restore_best_weights and best_model_weights:\n",
    "                    print(\"Restoring best model weights...\")\n",
    "                    model.load_state_dict(best_model_weights)\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea860022-67dd-4e33-8788-a4ad1351f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = early_stopping(patience=3, min_delta=0.01, restore_best_weights=True, start_from_epoch=0)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "121b4153-e18c-4fd4-8eeb-15fdb1629b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    all_losses = []\n",
    "    all_losses_dict = []\n",
    "    \n",
    "    for images, targets in tqdm(loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets) \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        all_losses.append(loss_value)\n",
    "        all_losses_dict.append(loss_dict_append)\n",
    "        \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping trainig\") \n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    all_losses_dict = pd.DataFrame(all_losses_dict) \n",
    "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
    "        all_losses_dict['loss_classifier'].mean(),\n",
    "        all_losses_dict['loss_box_reg'].mean(),\n",
    "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
    "        all_losses_dict['loss_objectness'].mean()\n",
    "    ))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f3dad-7d10-4150-92e4-18b9a94babed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # No gradients needed for evaluation\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for images, targets in loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        # Get model predictions (model is in eval mode, so it won't return loss)\n",
    "        outputs = model(images)  \n",
    "\n",
    "        # Compute validation loss if needed (requires a loss function)\n",
    "        if \"loss_classifier\" in outputs[0]:  # Ensure losses exist\n",
    "            loss_dict = outputs\n",
    "            loss = sum(loss for loss in loss_dict.values()).item()\n",
    "        else:\n",
    "            loss = 0  # If no loss, just set to 0\n",
    "\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / max(num_batches, 1)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef258b9f-0280-4241-8286-5fcca4fdae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1253 [00:00<?, ?it/s]/tmp/ipykernel_6635/549644185.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1253/1253 [01:34<00:00, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, lr: 0.010000, loss: 0.789876, loss_classifier: 0.316901, loss_box: 0.337475, loss_rpn_box: 0.076252, loss_object: 0.059248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluate validation loss\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m(model, test_loader, device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Check early stopping condition\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_early(epoch, val_loss, model):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs=5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Evaluate validation loss\n",
    "    \n",
    "    val_loss = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # Check early stopping condition\n",
    "    if stop_early(epoch, val_loss, model):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e5eaf-612a-48a5-b757-5f2184439f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"‚úÖ Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fb63b-91e1-4564-8b15-985e44fbb16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1b7a7-70d8-4ecb-8ffe-3ec415759fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ObjectDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd3dd3-e3a6-420c-9c85-1e775180c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device, iou_threshold=0.5):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_ap = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader, desc=\"Evaluating\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            predictions = model(images)\n",
    "\n",
    "            for pred, target in zip(predictions, targets):\n",
    "                pred_boxes = pred['boxes'].cpu()\n",
    "                pred_scores = pred['scores'].cpu()\n",
    "                pred_labels = pred['labels'].cpu()\n",
    "\n",
    "                true_boxes = target['boxes'].cpu()\n",
    "                true_labels = target['labels'].cpu()\n",
    "\n",
    "                if len(pred_boxes) > 0 and len(true_boxes) > 0:\n",
    "                    iou_matrix = box_iou(pred_boxes, true_boxes)  \n",
    "                    max_iou, max_iou_idx = iou_matrix.max(dim=1)  \n",
    "                    correct = (max_iou > iou_threshold) & (pred_labels == true_labels[max_iou_idx])\n",
    "                    ap = correct.float().sum() / max(1, len(true_boxes))  \n",
    "                else:\n",
    "                    ap = torch.tensor(0.0)  \n",
    "\n",
    "                all_ap.append(ap.item())\n",
    "\n",
    "    mAP = sum(all_ap) / len(all_ap)\n",
    "    print(f\"mAP@{iou_threshold}: {mAP:.4f}\")\n",
    "    return mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24798f45-7b2f-48a3-8773-64a1b33efefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a58778-a292-4e8b-9a28-fa16825fc27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP_score = evaluate(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc55d0-cbc1-4dd2-a958-7d1a8b8e4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "mAp_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quickml_env",
   "language": "python",
   "name": "quickml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
