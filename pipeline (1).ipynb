{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "978bd7a7-016b-43f4-a5e4-f0638073f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d670264-6483-4c67-8e18-09fe7964f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.ops.boxes import box_iou\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import xml.etree.ElementTree as ET\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "affe1d00-cc21-4528-95af-4b986cb9d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def pascal_voc_to_coco(voc_dir, coco_dir):\n",
    "    if not os.path.exists(coco_dir):\n",
    "        os.makedirs(coco_dir)\n",
    "\n",
    "    coco_annotations = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "\n",
    "    category_mapping = {}\n",
    "    annotation_id = 1\n",
    "\n",
    "    for xml_file in os.listdir(voc_dir):\n",
    "        if not xml_file.endswith('.xml'):\n",
    "            continue\n",
    "\n",
    "        tree = ET.parse(os.path.join(voc_dir, xml_file))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        filename = root.find('filename').text\n",
    "        image_path = os.path.join(voc_dir, filename)\n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        image_id = len(coco_annotations[\"images\"]) + 1\n",
    "\n",
    "        coco_annotations[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": filename,\n",
    "            \"width\": width,\n",
    "            \"height\": height\n",
    "        })\n",
    "\n",
    "        image.save(os.path.join(coco_dir, filename))\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            category = obj.find('name').text\n",
    "            if category not in category_mapping:\n",
    "                category_id = len(category_mapping) + 1\n",
    "                category_mapping[category] = category_id\n",
    "                coco_annotations[\"categories\"].append({\n",
    "                    \"id\": category_id,\n",
    "                    \"name\": category\n",
    "                })\n",
    "            else:\n",
    "                category_id = category_mapping[category]\n",
    "\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "            coco_annotations[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [xmin, ymin, xmax - xmin, ymax - ymin],\n",
    "                \"area\": (xmax - xmin) * (ymax - ymin),\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    with open(os.path.join(coco_dir, 'annotations.json'), 'w') as f:\n",
    "        json.dump(coco_annotations, f, indent=4)'''\n",
    "\n",
    "#Modified code to handle VOC_2007 xml structure\n",
    "def pascal_voc_to_coco(voc_annotation_dir, voc_image_dir, coco_dir, max_files=10):\n",
    "    if not os.path.exists(coco_dir):\n",
    "        os.makedirs(coco_dir)\n",
    "\n",
    "    coco_annotations = {\"images\": [], \"annotations\": [], \"categories\": []}\n",
    "    category_mapping = {}\n",
    "    annotation_id = 1\n",
    "\n",
    "    xml_files = [f for f in os.listdir(voc_annotation_dir) if f.endswith('.xml')]\n",
    "    print(f\"üîç Found {len(xml_files)} XML annotation files.\")\n",
    "\n",
    "    if len(xml_files) == 0:\n",
    "        print(\"‚ùå No XML files found. Check your VOC dataset path.\")\n",
    "        return\n",
    "\n",
    "    for idx, xml_file in enumerate(xml_files):\n",
    "        xml_path = os.path.join(voc_annotation_dir, xml_file)\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error parsing {xml_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        filename = root.find('filename').text  # e.g., '000005.jpg'\n",
    "        \n",
    "        # Check if the image exists\n",
    "        image_path = None\n",
    "        for ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            temp_path = os.path.join(voc_image_dir, filename.replace(\".jpg\", ext))\n",
    "            if os.path.exists(temp_path):\n",
    "                image_path = temp_path\n",
    "                break\n",
    "\n",
    "        if image_path is None:\n",
    "            print(f\"‚ö†Ô∏è Warning: Image {filename} not found in {voc_image_dir}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Load image size\n",
    "        image = Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        image_id = idx + 1\n",
    "\n",
    "        coco_annotations[\"images\"].append({\n",
    "            \"id\": image_id, \"file_name\": filename, \"width\": width, \"height\": height\n",
    "        })\n",
    "\n",
    "        image.save(os.path.join(coco_dir, filename))  # Save images in COCO folder\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            category = obj.find('name').text\n",
    "            if category not in category_mapping:\n",
    "                category_id = len(category_mapping) + 1\n",
    "                category_mapping[category] = category_id\n",
    "                coco_annotations[\"categories\"].append({\"id\": category_id, \"name\": category})\n",
    "            else:\n",
    "                category_id = category_mapping[category]\n",
    "\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin, ymin, xmax, ymax = (int(bndbox.find(pos).text) for pos in ['xmin', 'ymin', 'xmax', 'ymax'])\n",
    "\n",
    "            if xmax <= xmin or ymax <= ymin:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid bbox in {xml_file}: [{xmin}, {ymin}, {xmax}, {ymax}]\")\n",
    "                continue\n",
    "\n",
    "            coco_annotations[\"annotations\"].append({\n",
    "                \"id\": annotation_id, \"image_id\": image_id, \"category_id\": category_id,\n",
    "                \"bbox\": [xmin, ymin, xmax - xmin, ymax - ymin], \"area\": (xmax - xmin) * (ymax - ymin), \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    with open(os.path.join(coco_dir, 'annotations_test.json'), 'w') as f:\n",
    "        json.dump(coco_annotations, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Successfully converted {len(xml_files)} VOC files to COCO format.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1079db49-c728-432f-9d31-20db9f378f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pascal_voc_to_coco('path_to_voc_dir', 'path_to_coco_dir')\n",
    "\n",
    "#pascal_voc_to_coco(annotations_dir, images_dir, coco_output_dir, max_files=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "46f19ed7-06f5-47e2-b15f-169ae2071810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img, bilateral_filter=False, median_filter=False, gaussian_filter=False):\n",
    "    \"\"\"\n",
    "    Preprocess an image by applying specified filters.\n",
    "\n",
    "    Parameters:\n",
    "        img_path (str): Path to the input image.\n",
    "        bilateral_filter (bool): Apply bilateral filter if True.\n",
    "        median_filter (bool): Apply median filter if True.\n",
    "        gaussian_filter (bool): Apply Gaussian filter if True.\n",
    "\n",
    "    Returns:\n",
    "        processed_img: The processed image after applying the filters.\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    '''mg = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found at the specified path.\")'''\n",
    "\n",
    "    processed_img = img\n",
    "\n",
    "    # Apply bilateral filter\n",
    "    if bilateral_filter:\n",
    "        processed_img = cv2.bilateralFilter(processed_img, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Apply median filter\n",
    "    if median_filter:\n",
    "        processed_img = cv2.medianBlur(processed_img, ksize=5)\n",
    "\n",
    "    # Apply Gaussian filter\n",
    "    if gaussian_filter:\n",
    "        processed_img = cv2.GaussianBlur(processed_img, ksize=(5, 5), sigmaX=0)\n",
    "\n",
    "    return processed_img    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74b877-e206-4d64-8891-ecb86283c09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4970898e-12c9-4175-8386-7ccca92632a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train=False):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.ColorJitter(p=0.2),\n",
    "            A.Transpose(p=0.2),\n",
    "            ToTensorV2()], \n",
    "            bbox_params=A.BboxParams(format='coco'))\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(256, 256),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7f78df48-9756-4d9f-bc01-2913ab0efa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class Detection(datasets.VisionDataset):\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.split = split #train, valid, test\n",
    "        self.coco = COCO(os.path.join(root, split, \"annotations.json\")) \n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "    \n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "        \n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target] \n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes)\n",
    "        \n",
    "        image = transformed['image']\n",
    "        boxes = transformed['bboxes']\n",
    "        \n",
    "        new_boxes = [] # convert from xywh to xyxy\n",
    "        for box in boxes:\n",
    "            xmin = box[0]\n",
    "            xmax = xmin + box[2]\n",
    "            ymin = box[1]\n",
    "            ymax = ymin + box[3]\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "        \n",
    "        targ = {} \n",
    "        targ['boxes'] = boxes\n",
    "        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
    "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) \n",
    "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "        return image.div(255), targ \n",
    "    def __len__(self):\n",
    "        return len(self.ids)'''\n",
    "\n",
    "class ObjectDetection(datasets.VisionDataset):\n",
    "    def __init__(self, root, split='train', transform=None, preprocess_params=None):\n",
    "        self.root = root  # Root directory containing images and annotations\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.preprocess_params = preprocess_params or {}\n",
    "        self.coco = COCO(os.path.join(root, split, \"annotations.json\"))  # No train/valid split\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "        # Filter out images with no annotations\n",
    "        self.ids = [id for id in self.ids if len(self._load_target(id)) > 0]\n",
    "\n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root,self.split, path))  # Directly in root, not subdir\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        \n",
    "        #Preprocess function\n",
    "        if self.preprocess_params:\n",
    "            image = preprocess_image(image, **self.preprocess_params)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)  # Returns a NumPy array\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "\n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, bboxes=boxes)\n",
    "            image = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "\n",
    "        new_boxes = []  # Convert from xywh to xyxy\n",
    "        for box in boxes:\n",
    "            xmin, ymin, w, h, category_id = box\n",
    "            xmax = xmin + w\n",
    "            ymax = ymin + h\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "\n",
    "        targ = {\n",
    "            'boxes': boxes,\n",
    "            'labels': torch.tensor([t['category_id'] for t in target], dtype=torch.int64),\n",
    "            'image_id': torch.tensor([id], dtype=torch.int64),\n",
    "            'area': (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]),\n",
    "            'iscrowd': torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        # **Convert NumPy image to PyTorch tensor & Change Shape [H, W, C] -> [C, H, W]**\n",
    "        #image = torch.tensor(image, dtype=torch.float32).permute(1,0,2)  # Convert [H, W, C] ‚Üí [C, H, W]\n",
    "        \n",
    "        \n",
    "        return image / 255.0, targ  # Normalize image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc7a8aa4-b6f2-4253-b5e5-6659b3563742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = \"/work/qucikml/VOC_2007\"\n",
    "\n",
    "preprocess_params = {\n",
    "     'bilateral_filter': True,  # Apply bilateral filter\n",
    "     'median_filter': False,    # Do not apply median filter\n",
    "     'gaussian_filter': True,   # Apply Gaussian filter\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6fed2206-9ca0-4b2d-bd62-f24c4cfcf16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = AquariumDetection(root=dataset_path, transforms=get_transforms(True))\n",
    "#train_dataset = CocoDetection(root=dataset_path, annFile=annotations_file, transform=get_transforms(True))\n",
    "train_dataset = ObjectDetection(root=dataset_path, split=\"train\", transform=get_transforms(True),preprocess_params=preprocess_params)\n",
    "# fulltest_dataset = ObjectDetection(root=dataset_path, split=\"test\", transform=None)\n",
    "val_dataset = ObjectDetection(root=dataset_path, split=\"test\", transform=None)\n",
    "\n",
    "# Define train-validation split ratio\n",
    "# val_size = int(0.2 * len(fulltest_dataset))  # 20% for validation\n",
    "# test_size = len(fulltest_dataset) - val_size\n",
    "\n",
    "# Split the dataset\n",
    "# test_dataset, val_dataset = random_split(fulltest_dataset, [test_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "06a8f51e-5fb8-4968-821f-331d5892a8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of Classes: 21\n"
     ]
    }
   ],
   "source": [
    "'''coco = COCO(os.path.join(dataset_path, \"train\", \"annotations.json\"))\n",
    "categories = coco.cats\n",
    "n_classes = len(categories.keys())'''\n",
    "coco = COCO(os.path.join(dataset_path,\"train\", \"annotations.json\"))\n",
    "categories = coco.cats\n",
    "n_classes = len(categories.keys())+1\n",
    "\n",
    "print(f\"Number of Classes: {n_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fea95e3b-9060-4a03-9db3-e8253a53d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "363220d7-0457-4928-aff4-599f266263a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.roi_heads.nms_thresh = 0.5\n",
    "model.roi_heads.score_thresh = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "95e22982-e4d5-4a90-bd7b-f74b7eb9f9c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d00f7e65-f4e0-442e-a54d-6441ec4e6ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=True)\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "anchor_sizes = ((32,), (64,), (128,), (256,), (512,))  \n",
    "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)  \n",
    "anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
    "model.rpn.anchor_generator = anchor_generator\n",
    "model.rpn.nms_thresh = 0.7\n",
    "model.roi_heads.nms_thresh = 0.5\n",
    "model.roi_heads.score_thresh = 0.05\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features \n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "209f548a-7730-45a1-b120-fe946ab6386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "82ffd2a7-8641-4faf-9516-6f3251b38054",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3505f057-20cd-487c-b2a6-2e3af39ecbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ObjectDetection\n",
       "    Number of datapoints: 5011\n",
       "    Root location: /work/qucikml/VOC_2007"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e489092b-da6b-4956-b013-2c92b001921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,targets = next(iter(train_loader))\n",
    "images = list(image for image in images)\n",
    "#targets = [{k: torch.as_tensor(v, dtype=torch.int64).to(device) for k, v in t.items()} for t in targets]\n",
    "targets = [{k:v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e5b9efbf-da80-421b-9882-85a1a158de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images,targets = next(iter(train_loader))\n",
    "# print(images[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# images = list(image for image in images)\n",
    "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# targets = [{k:v for k, v in t.items()} for t in targets]\n",
    "# image = torch.tensor(image, dtype=torch.float32).permute(2,0,1)  # Convert [H, W, C] ‚Üí [C, H, W]\n",
    "\n",
    "\n",
    "# print(images[0].shape)'''\n",
    "\n",
    "# #targets = [{k: torch.as_tensor(v, dtype=torch.int64).to(device) for k, v in t.items()} for t in targets]\n",
    "# #targets = [{k:v for k, v in t.items()} for t in targets]\n",
    "# output = model(images, targets) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "77567673-4d61-4a1e-97de-af15074894c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ff60f5ab-0856-4e1c-a1f5-d25c761bd8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a1c20ca9-ad26-4fe4-9594-9ce4949de3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(patience=3, min_delta=0.0, restore_best_weights=True, start_from_epoch=0):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    def check(epoch, val_loss, model):\n",
    "        nonlocal best_loss, patience_counter, best_model_weights\n",
    "        \n",
    "        if epoch < start_from_epoch:\n",
    "            return False\n",
    "        \n",
    "        if val_loss < best_loss - min_delta:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            if restore_best_weights:\n",
    "                best_model_weights = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                if restore_best_weights and best_model_weights:\n",
    "                    print(\"Restoring best model weights...\")\n",
    "                    model.load_state_dict(best_model_weights)\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    return check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ea860022-67dd-4e33-8788-a4ad1351f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = early_stopping(patience=3, min_delta=0.01, restore_best_weights=True, start_from_epoch=0)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "121b4153-e18c-4fd4-8eeb-15fdb1629b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    all_losses = []\n",
    "    all_losses_dict = []\n",
    "    \n",
    "    for images, targets in tqdm(loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets) \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        all_losses.append(loss_value)\n",
    "        all_losses_dict.append(loss_dict_append)\n",
    "        \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping trainig\") \n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    all_losses_dict = pd.DataFrame(all_losses_dict) \n",
    "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
    "        all_losses_dict['loss_classifier'].mean(),\n",
    "        all_losses_dict['loss_box_reg'].mean(),\n",
    "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
    "        all_losses_dict['loss_objectness'].mean()\n",
    "    ))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "251f3dad-7d10-4150-92e4-18b9a94babed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # No gradients needed for evaluation\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for images, targets in loader:\n",
    "        images = [image.permute(1, 0, 2).float() for image in images]\n",
    "        images = [img.to(device) for img in images]\n",
    "        \n",
    "        # Get model predictions (model is in eval mode, so it won't return loss)\n",
    "        outputs = model(images)  \n",
    "\n",
    "        # Compute validation loss if needed (requires a loss function)\n",
    "        if \"loss_classifier\" in outputs[0]:  # Ensure losses exist\n",
    "            loss_dict = outputs\n",
    "            loss = sum(loss for loss in loss_dict.values()).item()\n",
    "        else:\n",
    "            loss = 0  # If no loss, just set to 0\n",
    "\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / max(num_batches, 1)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef258b9f-0280-4241-8286-5fcca4fdae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1253 [00:00<?, ?it/s]/tmp/ipykernel_926/456005237.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
      " 14%|‚ñà‚ñç        | 174/1253 [00:31<03:12,  5.59it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs=20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Evaluate validation loss\n",
    "    \n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Check early stopping condition\n",
    "    if stop_early(epoch, val_loss, model):\n",
    "       break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e5eaf-612a-48a5-b757-5f2184439f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/work/qucikml/model.pth\")\n",
    "print(\"‚úÖ Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fb63b-91e1-4564-8b15-985e44fbb16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1b7a7-70d8-4ecb-8ffe-3ec415759fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ObjectDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd3dd3-e3a6-420c-9c85-1e775180c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate1(model, loader, device, iou_threshold=0.5):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_ap = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader, desc=\"Evaluating\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "           \n",
    "\n",
    "            predictions = model(images)\n",
    "\n",
    "            for pred, target in zip(predictions, targets):\n",
    "                pred_boxes = pred['boxes'].cpu()\n",
    "                pred_scores = pred['scores'].cpu()\n",
    "                pred_labels = pred['labels'].cpu()\n",
    "\n",
    "                true_boxes = target['boxes'].cpu()\n",
    "                true_labels = target['labels'].cpu()\n",
    "\n",
    "                if len(pred_boxes) > 0 and len(true_boxes) > 0:\n",
    "                    iou_matrix = box_iou(pred_boxes, true_boxes)  \n",
    "                    max_iou, max_iou_idx = iou_matrix.max(dim=1)  \n",
    "                    correct = (max_iou > iou_threshold) & (pred_labels == true_labels[max_iou_idx])\n",
    "                    ap = correct.float().sum() / max(1, len(true_boxes))  \n",
    "                else:\n",
    "                    ap = torch.tensor(0.0)  \n",
    "\n",
    "                all_ap.append(ap.item())\n",
    "\n",
    "    mAP = sum(all_ap) / len(all_ap)\n",
    "    print(f\"mAP@{iou_threshold}: {mAP:.4f}\")\n",
    "    return mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24798f45-7b2f-48a3-8773-64a1b33efefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a58778-a292-4e8b-9a28-fa16825fc27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP_score = evaluate1(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc55d0-cbc1-4dd2-a958-7d1a8b8e4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "mAp_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b1f8d-8c85-4587-9d07-43bb2b527244",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48cc28-574f-47d7-a778-9f0f2b7ec417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_and_copy_data(test_dir, output_dir, annotation_file, split_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Split the test directory into two subdirectories: test1 and val1, with an 80-20 ratio\n",
    "    and copy the corresponding annotations to new files.\n",
    "    \n",
    "    Parameters:\n",
    "        test_dir (str): Path to the original test directory containing images and annotations.\n",
    "        output_dir (str): Path to the output directory where test1 and val1 will be created.\n",
    "        annotation_file (str): Path to the original annotation file (COCO format).\n",
    "        split_ratio (float): The ratio for the train-val split, default is 0.2 (i.e., 80-20 split).\n",
    "    \"\"\"\n",
    "    # Ensure the output directories exist\n",
    "    test1_dir = os.path.join(output_dir, 'test1')\n",
    "    val1_dir = os.path.join(output_dir, 'val1')\n",
    "\n",
    "    os.makedirs(test1_dir, exist_ok=True)\n",
    "    os.makedirs(val1_dir, exist_ok=True)\n",
    "\n",
    "    # Load the annotations file (COCO format)\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    # Get all image ids and shuffle them for random selection\n",
    "    all_image_ids = [image['id'] for image in annotations['images']]\n",
    "    random.shuffle(all_image_ids)\n",
    "\n",
    "    # Calculate the split index for the 80-20 split\n",
    "    split_index = int(len(all_image_ids) * (1 - split_ratio))\n",
    "    test_image_ids = set(all_image_ids[:split_index])\n",
    "    val_image_ids = set(all_image_ids[split_index:])\n",
    "\n",
    "    # Copy the images to the new directories\n",
    "    for image in tqdm(annotations['images'], desc=\"Processing Images\"):\n",
    "        image_id = image['id']\n",
    "        filename = image['file_name']\n",
    "\n",
    "        # Determine the new directory based on the split\n",
    "        if image_id in test_image_ids:\n",
    "            new_image_path = os.path.join(test1_dir, filename)\n",
    "        else:\n",
    "            new_image_path = os.path.join(val1_dir, filename)\n",
    "\n",
    "        # Copy the image to the appropriate directory\n",
    "        original_image_path = os.path.join(test_dir, filename)\n",
    "        shutil.copy(original_image_path, new_image_path)\n",
    "\n",
    "    # Update the annotations for the new directories (test1 and val1)\n",
    "    new_annotations_test1 = {\"images\": [], \"annotations\": [], \"categories\": annotations['categories']}\n",
    "    new_annotations_val1 = {\"images\": [], \"annotations\": [], \"categories\": annotations['categories']}\n",
    "\n",
    "    for image in annotations['images']:\n",
    "        image_id = image['id']\n",
    "        filename = image['file_name']\n",
    "\n",
    "        # Update file paths based on whether the image is in test1 or val1\n",
    "        if image_id in test_image_ids:\n",
    "            image['file_name'] = os.path.join('test1', filename)\n",
    "            new_annotations_test1[\"images\"].append(image)\n",
    "        else:\n",
    "            image['file_name'] = os.path.join('val1', filename)\n",
    "            new_annotations_val1[\"images\"].append(image)\n",
    "\n",
    "    # Copy the annotations for the images to the appropriate lists\n",
    "    for annotation in annotations['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        \n",
    "        # Add annotations to the respective list based on image id\n",
    "        if image_id in test_image_ids:\n",
    "            new_annotations_test1[\"annotations\"].append(annotation)\n",
    "        else:\n",
    "            new_annotations_val1[\"annotations\"].append(annotation)\n",
    "\n",
    "    # Save the updated annotations to new files\n",
    "    test1_annotations_file = os.path.join(output_dir, 'annotations_test1.json')\n",
    "    val1_annotations_file = os.path.join(output_dir, 'annotations_val1.json')\n",
    "\n",
    "    with open(test1_annotations_file, 'w') as f:\n",
    "        json.dump(new_annotations_test1, f, indent=4)\n",
    "\n",
    "    with open(val1_annotations_file, 'w') as f:\n",
    "        json.dump(new_annotations_val1, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Successfully created 'test1' and 'val1' directories with {len(test_image_ids)} test and {len(val_image_ids)} val images.\")\n",
    "\n",
    "# Example usage\n",
    "test_directory = '/work/qucikml/VOC_2007/test'  # Replace with your test directory\n",
    "output_directory = '/work/qucikml/VOC_2008'  # Replace with your output directory\n",
    "annotations_file = '/work/qucikml/VOC_2007/test/annotations.json'  # Path to the annotations file\n",
    "\n",
    "# Call the function to split the data and copy the images and annotations\n",
    "split_and_copy_data(test_directory, output_directory, annotations_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a418b-f8a1-4419-91ff-221ec82d904f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quickml_env",
   "language": "python",
   "name": "quickml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
